from openai import OpenAI

# replace it with OpenAI Key
OPENAI_API_KEY = ""


def chat_content(**kwargs):
    """
    Available arguments:
        messages: A list of messages comprising the conversation so far. Depending on the
            [model](https://platform.openai.com/docs/models) you use, different message
            types (modalities) are supported, like
            [text](https://platform.openai.com/docs/guides/text-generation),
            [images](https://platform.openai.com/docs/guides/vision), and
            [audio](https://platform.openai.com/docs/guides/audio).

        model: Model ID used to generate the response, like `gpt-4o` or `o1`. OpenAI offers a
            wide range of models with different capabilities, performance characteristics,
            and price points. Refer to the
            [model guide](https://platform.openai.com/docs/models) to browse and compare
            available models.

        audio: Parameters for audio output. Required when audio output is requested with
            `modalities: ["audio"]`.
            [Learn more](https://platform.openai.com/docs/guides/audio).

        frequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their
            existing frequency in the text so far, decreasing the model's likelihood to
            repeat the same line verbatim.

        function_call: Deprecated in favor of `tool_choice`.

            Controls which (if any) function is called by the model.

            `none` means the model will not call a function and instead generates a message.

            `auto` means the model can pick between generating a message or calling a
            function.

            Specifying a particular function via `{"name": "my_function"}` forces the model
            to call that function.

            `none` is the default when no functions are present. `auto` is the default if
            functions are present.

        functions: Deprecated in favor of `tools`.

            A list of functions the model may generate JSON inputs for.

        logit_bias: Modify the likelihood of specified tokens appearing in the completion.

            Accepts a JSON object that maps tokens (specified by their token ID in the
            tokenizer) to an associated bias value from -100 to 100. Mathematically, the
            bias is added to the logits generated by the model prior to sampling. The exact
            effect will vary per model, but values between -1 and 1 should decrease or
            increase likelihood of selection; values like -100 or 100 should result in a ban
            or exclusive selection of the relevant token.

        logprobs: Whether to return log probabilities of the output tokens or not. If true,
            returns the log probabilities of each output token returned in the `content` of
            `message`.

        max_completion_tokens: An upper bound for the number of tokens that can be generated for a completion,
            including visible output tokens and
            [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).

        max_tokens: The maximum number of [tokens](/tokenizer) that can be generated in the chat
            completion. This value can be used to control
            [costs](https://openai.com/api/pricing/) for text generated via API.

            This value is now deprecated in favor of `max_completion_tokens`, and is not
            compatible with
            [o1 series models](https://platform.openai.com/docs/guides/reasoning).

        metadata: Set of 16 key-value pairs that can be attached to an object. This can be useful
            for storing additional information about the object in a structured format, and
            querying for objects via API or the dashboard.

            Keys are strings with a maximum length of 64 characters. Values are strings with
            a maximum length of 512 characters.

        modalities: Output types that you would like the model to generate. Most models are capable
            of generating text, which is the default:

            `["text"]`

            The `gpt-4o-audio-preview` model can also be used to
            [generate audio](https://platform.openai.com/docs/guides/audio). To request that
            this model generate both text and audio responses, you can use:

            `["text", "audio"]`

        n: How many chat completion choices to generate for each input message. Note that
            you will be charged based on the number of generated tokens across all of the
            choices. Keep `n` as `1` to minimize costs.

        parallel_tool_calls: Whether to enable
            [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)
            during tool use.

        prediction: Static predicted output content, such as the content of a text file that is
            being regenerated.

        presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on
            whether they appear in the text so far, increasing the model's likelihood to
            talk about new topics.

        reasoning_effort: **o-series models only**

            Constrains effort on reasoning for
            [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently
            supported values are `low`, `medium`, and `high`. Reducing reasoning effort can
            result in faster responses and fewer tokens used on reasoning in a response.

        response_format: An object specifying the format that the model must output.

            Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
            Outputs which ensures the model will match your supplied JSON schema. Learn more
            in the
            [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).

            Setting to `{ "type": "json_object" }` enables the older JSON mode, which
            ensures the message the model generates is valid JSON. Using `json_schema` is
            preferred for models that support it.

        seed: This feature is in Beta. If specified, our system will make a best effort to
            sample deterministically, such that repeated requests with the same `seed` and
            parameters should return the same result. Determinism is not guaranteed, and you
            should refer to the `system_fingerprint` response parameter to monitor changes
            in the backend.

        service_tier: Specifies the latency tier to use for processing the request. This parameter is
            relevant for customers subscribed to the scale tier service:

            - If set to 'auto', and the Project is Scale tier enabled, the system will
            utilize scale tier credits until they are exhausted.
            - If set to 'auto', and the Project is not Scale tier enabled, the request will
            be processed using the default service tier with a lower uptime SLA and no
            latency guarentee.
            - If set to 'default', the request will be processed using the default service
            tier with a lower uptime SLA and no latency guarentee.
            - When not set, the default behavior is 'auto'.

            When this parameter is set, the response body will include the `service_tier`
            utilized.

        stop: Up to 4 sequences where the API will stop generating further tokens. The
            returned text will not contain the stop sequence.

        store: Whether or not to store the output of this chat completion request for use in
            our [model distillation](https://platform.openai.com/docs/guides/distillation)
            or [evals](https://platform.openai.com/docs/guides/evals) products.

        stream: If set to true, the model response data will be streamed to the client as it is
            generated using
            [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).
            See the
            [Streaming section below](https://platform.openai.com/docs/api-reference/chat/streaming)
            for more information, along with the
            [streaming responses](https://platform.openai.com/docs/guides/streaming-responses)
            guide for more information on how to handle the streaming events.

        stream_options: Options for streaming response. Only set this when you set `stream: true`.

        temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
            make the output more random, while lower values like 0.2 will make it more
            focused and deterministic. We generally recommend altering this or `top_p` but
            not both.

        tool_choice: Controls which (if any) tool is called by the model. `none` means the model will
            not call any tool and instead generates a message. `auto` means the model can
            pick between generating a message or calling one or more tools. `required` means
            the model must call one or more tools. Specifying a particular tool via
            `{"type": "function", "function": {"name": "my_function"}}` forces the model to
            call that tool.

            `none` is the default when no tools are present. `auto` is the default if tools
            are present.

        tools: A list of tools the model may call. Currently, only functions are supported as a
            tool. Use this to provide a list of functions the model may generate JSON inputs
            for. A max of 128 functions are supported.

        top_logprobs: An integer between 0 and 20 specifying the number of most likely tokens to
            return at each token position, each with an associated log probability.
            `logprobs` must be set to `true` if this parameter is used.

        top_p: An alternative to sampling with temperature, called nucleus sampling, where the
            model considers the results of the tokens with top_p probability mass. So 0.1
            means only the tokens comprising the top 10% probability mass are considered.

            We generally recommend altering this or `temperature` but not both.

        user: A unique identifier representing your end-user, which can help OpenAI to monitor
            and detect abuse.
            [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).

        web_search_options: This tool searches the web for relevant results to use in a response. Learn more
            about the
            [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).

        extra_headers: Send extra headers

        extra_query: Add additional query parameters to the request

        extra_body: Add additional JSON properties to the request

        timeout: Override the client-level default timeout for this request, in seconds
    """
    # client = OpenAI(api_key=OPENAI_API_KEY)
    
    # using deepseek API key
    client = OpenAI(api_key=OPENAI_API_KEY, base_url="https://api.deepseek.com")
    response = client.chat.completions.create(model="gpt-4o-2024-08-06", **kwargs)
    return response.choices[0].message.content
